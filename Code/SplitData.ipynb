{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lag4yfNGRsiP",
        "outputId": "e5eb44ef-f1b2-463e-c421-f637cdbd3dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCiwnjPsg85w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24421d15-c1ad-4056-9af1-e41f8721daa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Scanning /content/drive/MyDrive/bitmex_incremental_book_L2.csv to analyze Market Regimes...\n",
            "‚úÖ Scan complete. Aggregating stats...\n",
            "\n",
            "üìä REGIME THRESHOLDS:\n",
            "   üü¢ Quiet (Low Vol):   < 0.431036\n",
            "   üü° Normal (Med Vol):  0.431036 - 0.460932\n",
            "   üî¥ Volatile (High):   > 0.460932\n",
            "\n",
            "üöÄ Starting Split Process into /content/drive/MyDrive/...\n",
            ".......................................\n",
            "\n",
            "‚úÖ Split complete. Uploading to Cloud Storage...\n",
            "   Uploading quiet_temp.csv -> /content/drive/MyDrive/regime_quiet.csv\n",
            "   Uploading normal_temp.csv -> /content/drive/MyDrive/regime_normal.csv\n",
            "   Uploading volatile_temp.csv -> /content/drive/MyDrive/regime_volatile.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# UPDATE THESE PATHS TO MATCH YOUR GCS BUCKET\n",
        "INPUT_FILE = \"/content/drive/MyDrive/bitmex_incremental_book_L2.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/\"\n",
        "CHUNK_SIZE = 500000  # Process 500k rows at a time to save RAM\n",
        "\n",
        "def calculate_hourly_volatility(file_path):\n",
        "    \"\"\"\n",
        "    Scans the file to calculate volatility (std dev of returns) for every hour.\n",
        "    \"\"\"\n",
        "    print(f\"üîç Scanning {file_path} to analyze Market Regimes...\")\n",
        "\n",
        "    hourly_stats = {}\n",
        "\n",
        "    # We only need timestamp and price for this pass\n",
        "    cols = ['timestamp', 'price']\n",
        "\n",
        "    # Iterate through file\n",
        "    for chunk in pd.read_csv(file_path, compression='gzip', usecols=cols, chunksize=CHUNK_SIZE):\n",
        "        # Convert microsecond timestamp to datetime\n",
        "        chunk['datetime'] = pd.to_datetime(chunk['timestamp'], unit='us')\n",
        "        chunk['hour_key'] = chunk['datetime'].dt.floor('h') # Group by Hour\n",
        "\n",
        "        # Calculate Log Returns: ln(Pt / Pt-1)\n",
        "        # (This is the standard financial metric for volatility)\n",
        "        chunk['price'] = chunk['price'].replace(0, np.nan).ffill() # Handle zero prices\n",
        "        chunk['log_ret'] = np.log(chunk['price'] / chunk['price'].shift(1)).fillna(0)\n",
        "\n",
        "        # Group by hour and store sum of squared returns (for variance/std calculation)\n",
        "        # We aggregate logic here to handle chunks spanning across hours\n",
        "        grouped = chunk.groupby('hour_key')['log_ret'].agg(['std', 'count'])\n",
        "\n",
        "        for timestamp, row in grouped.iterrows():\n",
        "            if timestamp not in hourly_stats:\n",
        "                hourly_stats[timestamp] = []\n",
        "            hourly_stats[timestamp].append(row['std'])\n",
        "\n",
        "    print(\"‚úÖ Scan complete. Aggregating stats...\")\n",
        "\n",
        "    # Average the std dev across chunks for the same hour (simplified approximation)\n",
        "    final_volatility = {}\n",
        "    for ts, stds in hourly_stats.items():\n",
        "        # Filter out NaN\n",
        "        valid_stds = [s for s in stds if not np.isnan(s)]\n",
        "        if valid_stds:\n",
        "            final_volatility[ts] = np.mean(valid_stds)\n",
        "\n",
        "    return pd.Series(final_volatility).sort_index()\n",
        "\n",
        "def split_dataset(input_path, output_dir, volatility_series):\n",
        "    \"\"\"\n",
        "    Splits the main dataset into 3 files based on the volatility thresholds.\n",
        "    \"\"\"\n",
        "    # 1. Define Thresholds (Quantiles)\n",
        "    # Bottom 33% = Quiet, Middle 33% = Normal, Top 33% = Volatile\n",
        "    q33 = volatility_series.quantile(0.33)\n",
        "    q66 = volatility_series.quantile(0.66)\n",
        "\n",
        "    print(f\"\\nüìä REGIME THRESHOLDS:\")\n",
        "    print(f\"   üü¢ Quiet (Low Vol):   < {q33:.6f}\")\n",
        "    print(f\"   üü° Normal (Med Vol):  {q33:.6f} - {q66:.6f}\")\n",
        "    print(f\"   üî¥ Volatile (High):   > {q66:.6f}\")\n",
        "\n",
        "    # Map every hour to a label: 0=Quiet, 1=Normal, 2=Volatile\n",
        "    hour_to_regime = {}\n",
        "    for ts, vol in volatility_series.items():\n",
        "        if vol < q33:\n",
        "            hour_to_regime[ts] = 'quiet'\n",
        "        elif vol < q66:\n",
        "            hour_to_regime[ts] = 'normal'\n",
        "        else:\n",
        "            hour_to_regime[ts] = 'volatile'\n",
        "\n",
        "    print(f\"\\nüöÄ Starting Split Process into {output_dir}...\")\n",
        "\n",
        "    # Prepare Output Paths\n",
        "    paths = {\n",
        "        'quiet': f\"{output_dir}regime_quiet.csv\",\n",
        "        'normal': f\"{output_dir}regime_normal.csv\",\n",
        "        'volatile': f\"{output_dir}regime_volatile.csv\"\n",
        "    }\n",
        "\n",
        "    # Initialize files (write headers)\n",
        "    first_chunk = True\n",
        "\n",
        "    for chunk in pd.read_csv(input_path, compression='gzip', chunksize=CHUNK_SIZE):\n",
        "        # Calculate Hour Key\n",
        "        temp_dt = pd.to_datetime(chunk['timestamp'], unit='us')\n",
        "        chunk['hour_key'] = temp_dt.dt.floor('h')\n",
        "\n",
        "        # Map the regime\n",
        "        chunk['regime'] = chunk['hour_key'].map(hour_to_regime)\n",
        "\n",
        "        # Drop helper columns if you don't want them in final data\n",
        "        chunk = chunk.drop(columns=['hour_key'])\n",
        "\n",
        "        # Split and Append\n",
        "        for regime_name, path in paths.items():\n",
        "            subset = chunk[chunk['regime'] == regime_name]\n",
        "\n",
        "            if not subset.empty:\n",
        "                # Remove the 'regime' column before saving (optional)\n",
        "                save_data = subset.drop(columns=['regime'])\n",
        "\n",
        "                # If first time, write header. Else append.\n",
        "                mode = 'w' if first_chunk else 'a'\n",
        "                header = first_chunk\n",
        "\n",
        "                # Note: GCS doesn't support 'append' mode easily in pandas.\n",
        "                # If writing to GCS directly, we usually write local temp files then upload.\n",
        "                # FOR GCS: We will append to local files, then upload at the end.\n",
        "                local_path = f\"{regime_name}_temp.csv\"\n",
        "                save_data.to_csv(local_path, mode=mode, header=header, index=False)\n",
        "\n",
        "        first_chunk = False\n",
        "        print(f\".\", end=\"\", flush=True)\n",
        "\n",
        "    print(\"\\n\\n‚úÖ Split complete. Uploading to Cloud Storage...\")\n",
        "\n",
        "    # Upload local temp files to GCS\n",
        "    import shutil\n",
        "    from google.cloud import storage\n",
        "\n",
        "    # Simple upload logic (assuming auth is set)\n",
        "    # Alternatively use gsutil cp via os.system\n",
        "    for regime_name, gcs_path in paths.items():\n",
        "        local_filename = f\"{regime_name}_temp.csv\"\n",
        "        if os.path.exists(local_filename):\n",
        "            print(f\"   Uploading {local_filename} -> {gcs_path}\")\n",
        "            os.system(f\"gsutil cp {local_filename} {gcs_path}\")\n",
        "            os.remove(local_filename) # Cleanup\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Calculate Volatility Map\n",
        "    vol_series = calculate_hourly_volatility(INPUT_FILE)\n",
        "\n",
        "    # 2. Split and Save\n",
        "    split_dataset(INPUT_FILE, OUTPUT_DIR, vol_series)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ezDCYVr7TG80"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}